Hi there ðŸ‘‹! This is our teams final project of CS257 in uchicago.

Here is our project's abstract:

Health is an important issue, but it is also impractical to visit the doctor for any minor discomfort. 

Therefore, access to reliable preliminary medical guidance could be very beneficial and large language models (LLMs) have the potential to bridge this gap 
by generating medical hypotheses and offering general guidance based on symptom descriptions. 

In this project, we investigate the feasibility of using LLMs for this purpose by training GPT2-large and LLaMa-1B-Base on r/AskDocs posts and responses from Reddit. 
For further exploration, we also compare and evaluate the output of models under various generation parameters. 

We find that while LLMs can generate decent output, we still need to find a balance between deterministic and creativity in order to achieve optimal results 
and setting repetition penalty and temperature can be helpful.
